{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTHvJYxFDUYQ",
        "outputId": "b41ed4eb-b711-4fdf-d7bd-7360e5b67c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Run once\n",
        "!pip install -q \"transformers>=4.57.0\" sentencepiece safetensors accelerate einops ftfy regex pillow requests streamlit pyngrok streamlit-chat\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # follow the prompt\n",
        "# Set storage path (change if you want)\n",
        "CHAT_STORAGE_DIR = \"/content/drive/MyDrive/kosmos2_streamlit_vqa\"\n",
        "import os\n",
        "os.makedirs(CHAT_STORAGE_DIR, exist_ok=True)\n",
        "print(\"Chat storage dir:\", CHAT_STORAGE_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbd5Wz1xDVe5",
        "outputId": "68b68ac6-a2c4-4024-fdf1-57afa6ed0946"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Chat storage dir: /content/drive/MyDrive/kosmos2_streamlit_vqa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')  # optional\n",
        "\n",
        "if HF_TOKEN:\n",
        "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "    print(\"HF_TOKEN loaded.\")\n",
        "else:\n",
        "    print(\"⚠️ HF_TOKEN not found in userdata. Public model loading may fail for private models or be rate-limited.\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    os.environ['NGROK_AUTH_TOKEN'] = NGROK_AUTH_TOKEN\n",
        "    print(\"NGROK token loaded.\")\n",
        "else:\n",
        "    print(\"NGROK token not found. Tunnel may still work but is less stable.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICAxchNFDVhu",
        "outputId": "ffbb0472-4c37-4099-a997-2daace2a4afd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF_TOKEN loaded.\n",
            "NGROK token loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > app.py <<'PY'\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoProcessor, Kosmos2ForConditionalGeneration\n",
        "\n",
        "# Optional nice chat UI component\n",
        "try:\n",
        "    from streamlit_chat import message as st_message\n",
        "except Exception:\n",
        "    st_message = None\n",
        "\n",
        "# Storage path\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/kosmos2_streamlit_vqa\"\n",
        "if Path(DRIVE_PATH).exists():\n",
        "    STORAGE_DIR = Path(DRIVE_PATH)\n",
        "else:\n",
        "    STORAGE_DIR = Path(\"/content\")\n",
        "HISTORY_FILE = STORAGE_DIR / \"kosmos2_vqa_chat_history.json\"\n",
        "\n",
        "# Ensure file exists\n",
        "if not HISTORY_FILE.exists():\n",
        "    HISTORY_FILE.write_text(\"[]\")\n",
        "\n",
        "# Load HF token from environment (set by Colab before launching)\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def load_model_and_processor(model_id=\"microsoft/kosmos-2-patch14-224\"):\n",
        "    kwargs = {}\n",
        "    if HF_TOKEN:\n",
        "        kwargs[\"use_auth_token\"] = HF_TOKEN\n",
        "    st.info(\"Loading model (this may take ~1-2 minutes)...\")\n",
        "    processor = AutoProcessor.from_pretrained(model_id, **kwargs)\n",
        "    model = Kosmos2ForConditionalGeneration.from_pretrained(model_id, **kwargs)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return processor, model, device\n",
        "\n",
        "def load_history():\n",
        "    try:\n",
        "        with open(HISTORY_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def save_history(history):\n",
        "    with open(HISTORY_FILE, \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "def append_to_history(entry):\n",
        "    h = load_history()\n",
        "    h.append(entry)\n",
        "    save_history(h)\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(page_title=\"KOSMOS-2 VQA\", layout=\"centered\")\n",
        "st.title(\"KOSMOS-2 VQA — Upload image & ask questions\")\n",
        "\n",
        "col1, col2 = st.columns([1,2])\n",
        "\n",
        "with col1:\n",
        "    st.header(\"Image\")\n",
        "    uploaded = st.file_uploader(\"Upload an image\", type=[\"jpg\",\"jpeg\",\"png\"])\n",
        "    if uploaded:\n",
        "        image = Image.open(uploaded).convert(\"RGB\")\n",
        "    else:\n",
        "        if st.button(\"Use sample snowman\"):\n",
        "            import requests\n",
        "            url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n",
        "            image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "        else:\n",
        "            image = None\n",
        "\n",
        "    if image:\n",
        "        st.image(image, use_column_width=True)\n",
        "\n",
        "with col2:\n",
        "    st.header(\"Chat / VQA\")\n",
        "    history = load_history()\n",
        "    # show previous messages\n",
        "    if history:\n",
        "        for item in history:\n",
        "            role = item.get(\"role\")\n",
        "            text = item.get(\"text\")\n",
        "            ts = item.get(\"time\", \"\")\n",
        "            if role == \"user\":\n",
        "                if st_message:\n",
        "                    st_message(text, is_user=True, avatar_style=\"micah\")\n",
        "                else:\n",
        "                    st.markdown(f\"**You:** {text}\")\n",
        "            else:\n",
        "                if st_message:\n",
        "                    st_message(text, is_user=False, avatar_style=\"bottts\")\n",
        "                else:\n",
        "                    st.markdown(f\"**KOSMOS-2:** {text}\")\n",
        "\n",
        "    user_q = st.text_input(\"Ask a question about the image (VQA):\", key=\"vqa_input\")\n",
        "    if st.button(\"Ask\", key=\"ask_btn\"):\n",
        "        if image is None:\n",
        "            st.warning(\"Please upload or choose a sample image first.\")\n",
        "        elif not user_q:\n",
        "            st.warning(\"Type a question before hitting Ask.\")\n",
        "        else:\n",
        "            # load model\n",
        "            processor, model, device = load_model_and_processor()\n",
        "            # prepare prompt; using grounding token like earlier\n",
        "            prompt = \"<grounding> \" + user_q\n",
        "            inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
        "            # move tensors\n",
        "            for k,v in inputs.items():\n",
        "                inputs[k] = v.to(device)\n",
        "            # generate\n",
        "            with st.spinner(\"Generating answer from KOSMOS-2...\"):\n",
        "                generated_ids = model.generate(\n",
        "                    pixel_values=inputs.get(\"pixel_values\"),\n",
        "                    input_ids=inputs.get(\"input_ids\"),\n",
        "                    attention_mask=inputs.get(\"attention_mask\"),\n",
        "                    image_embeds=None,\n",
        "                    image_embeds_position_mask=inputs.get(\"image_embeds_position_mask\"),\n",
        "                    use_cache=True,\n",
        "                    max_new_tokens=128,\n",
        "                )\n",
        "                generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "                # attempt structured postprocessing\n",
        "                try:\n",
        "                    caption, entities = processor.post_process_generation(generated_text)\n",
        "                    answer_text = caption if caption else generated_text\n",
        "                except Exception:\n",
        "                    answer_text = generated_text\n",
        "\n",
        "            # display\n",
        "            if st_message:\n",
        "                st_message(user_q, is_user=True, avatar_style=\"micah\")\n",
        "                st_message(answer_text, is_user=False, avatar_style=\"bottts\")\n",
        "            else:\n",
        "                st.markdown(f\"**You:** {user_q}\")\n",
        "                st.markdown(f\"**KOSMOS-2:** {answer_text}\")\n",
        "\n",
        "            # save to history\n",
        "            append_to_history({\n",
        "                \"role\": \"user\",\n",
        "                \"text\": user_q,\n",
        "                \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            })\n",
        "            append_to_history({\n",
        "                \"role\": \"assistant\",\n",
        "                \"text\": answer_text,\n",
        "                \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            })\n",
        "            st.success(\"Saved to chat history.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.write(\"Chat history stored at:\", str(HISTORY_FILE))\n",
        "PY\n"
      ],
      "metadata": {
        "id": "jPVBHIEzDVkp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start streamlit and ngrok tunnel from Colab\n",
        "import os, signal, subprocess, time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# set ngrok auth token if present\n",
        "NGROK_AUTH_TOKEN = os.environ.get(\"NGROK_AUTH_TOKEN\")\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# kill any running streamlit or ngrok (clean start)\n",
        "def kill_process_by_name(name):\n",
        "    try:\n",
        "        subprocess.run([\"pkill\", \"-f\", name], check=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "kill_process_by_name(\"streamlit\")\n",
        "kill_process_by_name(\"ngrok\")\n",
        "\n",
        "# open ngrok tunnel for streamlit port 8501\n",
        "public_url = ngrok.connect(8501, \"http\").public_url\n",
        "print(\"ngrok tunnel URL:\", public_url)\n",
        "\n",
        "# Run streamlit in background\n",
        "cmd = [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.runOnSave\", \"false\"]\n",
        "print(\"Launching Streamlit...\")\n",
        "proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid)\n",
        "\n",
        "# give app a moment to start\n",
        "time.sleep(4)\n",
        "print(\"Streamlit should be starting — open the ngrok URL above (it may take a few seconds).\")\n",
        "print(\"To stop the app, run: kill -9\", proc.pid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAJspdetDVmr",
        "outputId": "08fde340-8654-462d-ae77-5d4aec78ebd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok tunnel URL: https://daniele-ritziest-saundra.ngrok-free.dev\n",
            "Launching Streamlit...\n",
            "Streamlit should be starting — open the ngrok URL above (it may take a few seconds).\n",
            "To stop the app, run: kill -9 1363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit; !pkill -f ngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8Ztfo9lDVpC",
        "outputId": "73f2b375-f217-4375-f231-d161e1aeec92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjAeabQLDVr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lp2vyyU3DVuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwmJlEc7DVxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}